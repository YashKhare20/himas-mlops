name: HIMAS CI Tests

on:
  pull_request:
    branches:
      - main
      - feature_poc
    paths:
      - "PoC/Data-Pipeline/dags/**"
      - "PoC/Data-Pipeline/tests/**"
      - "PoC/Data-Pipeline/requirements.txt"
      - "PoC/Data-Pipeline/.dvc/**"

jobs:
  test:
    runs-on: ubuntu-latest
    defaults:
      run:
        working-directory: PoC/Data-Pipeline
    services:
      postgres:
        image: postgres:13
        env:
          POSTGRES_USER: airflow
          POSTGRES_PASSWORD: airflow
          POSTGRES_DB: airflow
        ports:
          - 5432:5432
        options: >-
          --health-cmd pg_isready
          --health-interval 10s
          --health-timeout 5s
          --health-retries 5

    steps:
      - name: Checkout code
        uses: actions/checkout@v3
        with:
          fetch-depth: 0 # Full history for DVC

      - name: Set up Python
        uses: actions/setup-python@v4
        with:
          python-version: "3.11"

      - name: Cache pip packages
        uses: actions/cache@v3
        with:
          path: ~/.cache/pip
          key: ${{ runner.os }}-pip-${{ hashFiles('PoC/Data-Pipeline/requirements.txt') }}
          restore-keys: |
            ${{ runner.os }}-pip-

      - name: Install dependencies
        run: |
          python -m pip install --upgrade pip
          pip install -r requirements.txt
          pip install apache-airflow==3.1.0 apache-airflow-providers-google==10.22.0
          pip install pylint flake8 pytest pytest-cov pytest-mock

      - name: Set environment variables
        run: |
          echo "AIRFLOW_HOME=${GITHUB_WORKSPACE}/PoC/Data-Pipeline/airflow" >> $GITHUB_ENV
          echo "AIRFLOW__CORE__DAGS_FOLDER=${GITHUB_WORKSPACE}/PoC/Data-Pipeline/dags" >> $GITHUB_ENV

      - name: Validate DAG Integrity
        run: |
          python -c "
          import sys
          sys.path.insert(0, '${GITHUB_WORKSPACE}/PoC/Data-Pipeline/dags')
          from airflow.models import DagBag

          dag_bag = DagBag(dag_folder='${GITHUB_WORKSPACE}/PoC/Data-Pipeline/dags', include_examples=False)

          if dag_bag.import_errors:
              print('DAG Import Errors:')
              for filename, error in dag_bag.import_errors.items():
                  print(f'{filename}: {error}')
              sys.exit(1)

          print(f'Successfully loaded {len(dag_bag.dags)} DAGs')
          print('DAG IDs:', list(dag_bag.dag_ids))
          "

      - name: Check for DVC initialization
        run: |
          if [ ! -d ".dvc" ]; then
            echo "Error: .dvc directory not found. Please run 'dvc init' first."
            exit 1
          fi
          echo "DVC directory found ✓"

      - name: Validate DVC configuration
        run: |
          if [ ! -f ".dvc/config" ]; then
            echo "Warning: .dvc/config not found"
          else
            echo "DVC config found ✓"
            cat .dvc/config
          fi

      - name: Check DVC files
        run: |
          dvc_files=$(find . -name "*.dvc" -type f)
          if [ -z "$dvc_files" ]; then
            echo "No .dvc files found"
          else
            echo "Found DVC files:"
            echo "$dvc_files"
          fi

      - name: Run unit tests with coverage
        run: |
          pytest tests/ -v --cov=dags --cov-report=term-missing --cov-report=xml

      - name: Upload coverage reports
        uses: codecov/codecov-action@v3
        with:
          file: ./coverage.xml
          flags: unittests
          name: codecov-umbrella

      - name: Code quality check with flake8
        run: |
          flake8 dags/ --count --select=E9,F63,F7,F82 --show-source --statistics
          flake8 dags/ --count --exit-zero --max-complexity=10 --max-line-length=127 --statistics

      - name: Summary
        if: always()
        run: |
          echo "## Test Summary" >> $GITHUB_STEP_SUMMARY
          echo "DAG validation completed" >> $GITHUB_STEP_SUMMARY
          echo "Unit tests completed" >> $GITHUB_STEP_SUMMARY
          echo "DVC checks completed" >> $GITHUB_STEP_SUMMARY